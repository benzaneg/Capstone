{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep as go_night_night\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLP tools\n",
    "\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "## SpaCy NLP tools\n",
    "\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "\n",
    "## EDA and extra imports\n",
    "\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "import pickle\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(df, col_name):\n",
    "    # spaCy's stop_words is not working well so had to create a new list of stop words\n",
    "    for word in nlp.Defaults.stop_words:\n",
    "        for w in (word, word[0].upper() + word[1:], word.upper()):\n",
    "            lex = nlp.vocab[w]\n",
    "            lex.is_stop = True\n",
    "\n",
    "    # Need to add Pron as a stop word\n",
    "    # which is from when the words get lemmatized, I, he, we she, all get turnd into Pron\n",
    "    nlp.vocab[\"pron\"].is_stop = True\n",
    "    nlp.vocab[\"united state\"].is_stop = True\n",
    "    nlp.vocab[\"american\"].is_stop = True\n",
    "    nlp.vocab[\"'s'\"].is_stop = True\n",
    "    \n",
    "    # Creating a lemma version of chapters\n",
    "    length_para = []\n",
    "    chapter_gensim_lemma = []\n",
    "    chapter_lemma = []\n",
    "    for chap in df[col_name]:\n",
    "        sent = chap.lower()\n",
    "        sent = nlp(sent)\n",
    "        sent = [token for token in sent if token.is_stop == False]\n",
    "        sent = [token.lemma_.lower().strip() for token in sent]\n",
    "        sent = [token for token in sent if token not in punctuations]\n",
    "        chapter_lemma.append(' '.join(sent))\n",
    "        length_para.append(len(sent))\n",
    "        print('done')\n",
    "    df['chapter_lemma'] = chapter_lemma\n",
    "    df['len'] = length_para"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of URL links to the Document Topics I will be scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_historical_documents = ['http://teachingamericanhistory.org/library/executive/',\n",
    "    'http://teachingamericanhistory.org/library/civil-rights/',\n",
    "    'http://teachingamericanhistory.org/library/modern/',\n",
    "    'http://teachingamericanhistory.org/library/america-between-world-wars/',\n",
    "    'http://teachingamericanhistory.org/library/civil-war/',\n",
    "    'http://teachingamericanhistory.org/library/antebellum/',\n",
    "    'http://teachingamericanhistory.org/library/expansion/',\n",
    "    'http://teachingamericanhistory.org/library/founding/',\n",
    "    'http://teachingamericanhistory.org/library/colonial/',\n",
    "    'http://teachingamericanhistory.org/library/progressive/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLP tools\n",
    "\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "## SpaCy NLP tools\n",
    "\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "\n",
    "## EDA and extra imports\n",
    "\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "import pickle\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# spaCy's stop_words is not working well so had to create a new list of stop words\n",
    "for word in nlp.Defaults.stop_words:\n",
    "    for w in (word, word[0].upper() + word[1:], word.upper()):\n",
    "        lex = nlp.vocab[w]\n",
    "        lex.is_stop = True\n",
    "\n",
    "# Need to add Pron as a stop word\n",
    "# which is from when the words get lemmatized, I, he, we she, all get turnd into Pron\n",
    "nlp.vocab[\"pron\"].is_stop = True\n",
    "nlp.vocab[\"united state\"].is_stop = True\n",
    "nlp.vocab[\"american\"].is_stop = True\n",
    "nlp.vocab[\"'s'\"].is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Scrape Teaching American History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Historical_Doc_Scrapper(hist_url_list):\n",
    "    doc_para_list = []\n",
    "    doc_date_list = []\n",
    "    doc_auth_list = []\n",
    "    doc_title_list = []\n",
    "    for hist_url in hist_url_list:\n",
    "        res = requests.get(hist_url)\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        directory = str(soup.find_all(class_ = 'document-cats'))\n",
    "        documents_link = re.findall(\"\"\"href=\"(.+)\">\"\"\", directory)\n",
    "        for item in documents_link:\n",
    "            page = requests.get(item)\n",
    "            soup_individ_era = BeautifulSoup(page.content, 'lxml')\n",
    "            time.sleep(1)\n",
    "            library_list = str(soup_individ_era.find(class_ = \"library-list\"))\n",
    "            doc_tags = re.findall(\"\"\"href=\"(http:\\/\\/[a-z.\\/-]+)\"\"\", library_list)\n",
    "            for tag in doc_tags:\n",
    "                page = requests.get(tag)\n",
    "                time.sleep(1)\n",
    "                soup_individ_doc = BeautifulSoup(page.content, 'lxml')\n",
    "                doc_title = [tag.text for tag in soup_individ_doc.find_all(class_ = \"post-title\")]\n",
    "                doc_auth = [tag.text for tag in soup_individ_doc.find_all(class_ = \"single-docinfo-author\")]\n",
    "                doc_date = [tag.text for tag in soup_individ_doc.find_all(class_ = \"single-docinfo-date\")]\n",
    "                doc_para = [tag.text for tag in soup_individ_doc.find_all('p')]\n",
    "                doc_para = ' '.join(doc_para[2:])\n",
    "                print(doc_auth)\n",
    "                doc_para_list.append(doc_para)\n",
    "                doc_auth_list.append(doc_auth)\n",
    "                doc_date_list.append(doc_date)\n",
    "                doc_title_list.append(doc_title)\n",
    "                \n",
    "    # Taking out html code, '\\xa0', and eliminating watermark            \n",
    "    new_doc_corpus_para = []\n",
    "    for doc in doc_para_list:\n",
    "        doc = re.sub('\\\\xa0', '', doc)\n",
    "        doc = doc[:-319]\n",
    "        new_doc_corpus_para.append(doc)\n",
    "    \n",
    "    # Creating a list of years\n",
    "    new_corpus_date = []\n",
    "    for doc in doc_date_list:\n",
    "        if len(doc) == 0:\n",
    "            new_corpus_date.append('[0]')\n",
    "        else:\n",
    "            doc = re.findall('\\d{4}', doc[0])\n",
    "            new_corpus_date.append(' '.join(doc))\n",
    "    corpus_year = []\n",
    "    for it in new_corpus_date:\n",
    "        try:\n",
    "            corpus_year.append(int(it))\n",
    "        except:\n",
    "            corpus_year.append(0)\n",
    "    \n",
    "    # Fixing some of the dataframe types as well as dropping duplicates\n",
    "    new_auth = []\n",
    "    for row in doc_auth_list:\n",
    "        new_auth.append(''.join(row))\n",
    "    new_title = []\n",
    "    for row in doc_title_list:\n",
    "        new_title.append(''.join(row))\n",
    "    new_date = []\n",
    "    for row in doc_date_list:\n",
    "        new_date.append(''.join(row))\n",
    "    \n",
    "    # Creating a lemma version of chapters\n",
    "    length_para = []\n",
    "    chapter_gensim_lemma = []\n",
    "    chapter_lemma = []\n",
    "    for chap in df_pri['doc']:\n",
    "        sent = chap.lower()\n",
    "        sent = nlp(sent)\n",
    "        sent = [token for token in sent if token.is_stop == False]\n",
    "        sent = [token.lemma_.lower().strip() for token in sent]\n",
    "        sent = [token for token in sent if token not in punctuations]\n",
    "        chapter_lemma.append(' '.join(sent))\n",
    "        length_para.append(len(sent))\n",
    "        print('done')\n",
    "    df_pri['chapter_lemma'] = chapter_lemma\n",
    "    df_pri['len'] = length_para\n",
    "    \n",
    "    return new_doc_corpus_para, new_date, corpus_year, new_auth, new_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the model on my list of links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['http://teachingamericanhistory.org/library/progressive/', 'http://google.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Theodore Roosevelt']\n",
      "['Theodore Roosevelt']\n",
      "['Theodore Roosevelt']\n",
      "['George Washington']\n",
      "['James Polk']\n",
      "['James Polk']\n",
      "['Theodore Roosevelt']\n",
      "['John Adams']\n",
      "['']\n",
      "['John Adams']\n",
      "['Theodore Roosevelt']\n",
      "['James Polk']\n",
      "['James Polk']\n",
      "['George Washington']\n",
      "['John Quincy Adams']\n",
      "['James Polk']\n",
      "['James Polk']\n",
      "['Theodore Roosevelt']\n",
      "['James Polk']\n",
      "['James Polk']\n",
      "['James Polk']\n",
      "['James Polk']\n",
      "['Theodore Roosevelt']\n",
      "['Theodore Roosevelt']\n",
      "['James Polk']\n",
      "['James Polk']\n",
      "['Theodore Roosevelt']\n",
      "['Theodore Roosevelt']\n",
      "['Theodore Roosevelt']\n",
      "['Theodore Roosevelt']\n",
      "['Theodore Roosevelt']\n",
      "['Theodore Roosevelt']\n",
      "['Theodore Roosevelt']\n",
      "['Theodore Roosevelt']\n",
      "['Theodore Roosevelt']\n",
      "['John Adams']\n",
      "['John Adams']\n",
      "['James Polk']\n",
      "['James Polk']\n",
      "['William Howard Taft']\n",
      "['William Howard Taft']\n",
      "['James Polk']\n",
      "['James Polk']\n",
      "['Calvin Coolidge']\n",
      "['Calvin Coolidge']\n",
      "['John Adams']\n",
      "['Calvin Coolidge']\n",
      "['John Adams']\n",
      "['Calvin Coolidge']\n",
      "['Calvin Coolidge']\n",
      "['John Quincy Adams']\n",
      "['John Adams']\n",
      "['Calvin Coolidge']\n",
      "['Calvin Coolidge']\n",
      "['John Adams']\n",
      "['Calvin Coolidge']\n",
      "['John Adams']\n",
      "['Calvin Coolidge']\n",
      "['John Adams']\n",
      "['Elihu Root']\n",
      "['Elihu Root']\n",
      "['Booker T. Washington']\n",
      "['Booker T. Washington']\n",
      "['Booker T. Washington']\n",
      "['Booker T. Washington']\n",
      "['Booker T. Washington']\n",
      "['Booker T. Washington']\n",
      "['Booker T. Washington']\n",
      "['Booker T. Washington']\n",
      "['Booker T. Washington']\n",
      "['Booker T. Washington']\n",
      "['Booker T. Washington']\n",
      "['Booker T. Washington']\n",
      "['Booker T. Washington']\n",
      "['Booker T. Washington']\n",
      "['Booker T. Washington']\n",
      "['Booker T. Washington']\n",
      "['Booker T. Washington']\n",
      "['Booker T. Washington']\n",
      "['Booker T. Washington']\n",
      "['W.E.B. Du Bois']\n",
      "['W.E.B. DuBois']\n",
      "['W.E.B. DuBois']\n",
      "['W.E.B. DuBois ']\n",
      "['W.E.B. DuBois ']\n",
      "['W.E.B. DuBois ']\n",
      "['W.E.B. DuBois']\n",
      "['W.E.B. DuBois']\n",
      "['W.E.B. DuBois']\n",
      "['W.E.B. DuBois']\n",
      "['W.E.B. DuBois ']\n",
      "['W.E.B. DuBois']\n",
      "['W.E.B. DuBois']\n",
      "['W.E.B. DuBois']\n",
      "['W.E.B. DuBois']\n",
      "['W.E.B. DuBois ']\n",
      "['W.E.B. DuBois ']\n",
      "['W.E.B. DuBois ']\n",
      "['W.E.B. DuBois']\n",
      "['W.E.B. DuBois ']\n",
      "['W.E.B. DuBois ']\n",
      "['W.E.B. DuBois']\n",
      "['W.E.B. DuBois']\n",
      "['W.E.B. DuBois']\n",
      "['W.E.B. DuBois']\n",
      "['Frederick Jackson Turner']\n",
      "['Frederick Jackson Turner']\n",
      "['Albert J. Beveridge']\n",
      "['Albert J. Beveridge']\n",
      "['Woodrow Wilson']\n",
      "['Woodrow Wilson']\n",
      "['Woodrow Wilson']\n",
      "['Woodrow Wilson']\n",
      "['Woodrow Wilson ']\n",
      "['Woodrow Wilson']\n",
      "['Woodrow Wilson']\n",
      "['Woodrow Wilson']\n",
      "['Woodrow Wilson']\n",
      "['Woodrow Wilson']\n",
      "['Woodrow Wilson']\n",
      "['Woodrow Wilson']\n",
      "['Woodrow Wilson']\n",
      "['Woodrow Wilson']\n",
      "['George Washington']\n",
      "['Woodrow Wilson']\n",
      "['John Adams']\n",
      "['Woodrow Wilson']\n",
      "['Woodrow Wilson']\n",
      "['Woodrow Wilson']\n",
      "['Professor Munsterberg']\n",
      "['John Adams']\n",
      "['Woodrow Wilson']\n",
      "['Woodrow Wilson']\n",
      "['John Adams']\n",
      "['John Adams']\n",
      "['Woodrow Wilson']\n",
      "['Abraham Lincoln']\n",
      "['Woodrow Wilson']\n",
      "['Robert M. LaFollette']\n",
      "['Robert M. LaFollette']\n",
      "['John Adams']\n",
      "['Colonel House']\n",
      "['Woodrow Wilson']\n",
      "['']\n",
      "['John Adams']\n",
      "['Woodrow Wilson']\n",
      "['Congressional Record']\n",
      "['Republican Senators']\n",
      "['John Adams']\n",
      "['John Adams']\n",
      "['William Jennings Bryan']\n",
      "['Secretary of State William Jennings Bryan']\n",
      "['Henry George']\n",
      "['T. Thomas Fortune']\n",
      "['Andrew Carnegie']\n",
      "['']\n",
      "['Jane Addams']\n",
      "['']\n",
      "['Justice Brown (Opinion of the Court), Justice Harlan (Dissenting)']\n",
      "['Henry Van Dyke, D.D., LL.D.']\n",
      "['']\n",
      "['John T. Morgan']\n",
      "['United States Senate']\n",
      "['Lincoln Steffens']\n",
      "['Governor James K. Vardaman (MS)']\n",
      "['John Dewey']\n",
      "['Jane Addams']\n",
      "['']\n",
      "['Charles Beard']\n",
      "['']\n",
      "['Colonol Edward House']\n",
      "['John Dewey']\n",
      "['John Dewey']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['Arthur Zimmermann']\n",
      "['']\n",
      "['']\n",
      "['Colonel House']\n",
      "['']\n",
      "['John Dewey']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['Joseph Ernest McAfee']\n",
      "['Congressional Record']\n",
      "['Harry Emerson Fosdick']\n",
      "['J. Gersham Machen']\n",
      "['J. Gresham Machen']\n",
      "['Justice Holmes']\n",
      "['']\n",
      "['']\n",
      "['John Dewey']\n",
      "['']\n",
      "['Charles A. Beard']\n",
      "['']\n",
      "['']\n",
      "['Eleanor Roosevelt']\n"
     ]
    }
   ],
   "source": [
    "prog_corpus_para, prog_corpus_date, prog_corpus_year, prog_corpus_auth, prog_corpus_title  = Historical_Doc_Scrapper(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle, if I desire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3240"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.dump(new_corpus_date_fix, open('./text/corpus_year_pri.txt', 'wb'))\n",
    "corpus_year_pri = pickle.load(open('./text/corpus_year_pri.txt', 'rb'))\n",
    "len(corpus_year_pri)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
